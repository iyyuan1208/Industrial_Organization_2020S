---
title: "project 1 HHI, MLE, and MOM"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(nleqslv)
library(readr)
```

## HHI and CR (market concentration) computation

```{r HHI}
share_airline <- c(18, 18, 17, 15, 6, 6, 4, 3, 3)
HHI_calculate <- function(x){
  HHI <- 0
  for (i in 1:length(x)){
    HHI = HHI + (x[i])^2
  }
  return(HHI)
}

HHI_calculate(share_airline)
# It returns 1268 for HHI.
```

```{r CR}
CR_calculate <- function(x,n){
  CR <- 0
  sort(x)
  for (i in 1:n){
    CR = CR + x[i]
  }
  return(CR)
}

CR_calculate(share_airline,2) # returns 36
CR_calculate(share_airline,4) # returns 68
```

## HHI and CR computation for more complex data

```{r}
library(readr)
market_share_data <- read_csv("market_share_data.csv")

# HHI function 
market_HHI <- function(x, ind){
  HHI <- 0
  for (i in 1:nrow(x)){
    if (x$industry[i] == ind){
      HHI = HHI + (x$market_share[i])^2
    }
  }
    return(HHI)}

# calculate HHI for the four industries
market_HHI(market_share_data, "Smartphone") # returns 1213
market_HHI(market_share_data, "Banking") # returns 354
market_HHI(market_share_data, "Airline") # returns 1268
market_HHI(market_share_data, "Browser") # returns 4286
```
```{r}
# CR function
market_CR <- function(x,ind,n){
  CR <- 0
  CR_vector <- c()
  for (i in 1:nrow(x)){
    if (x$industry[i] == ind){
      CR_vector <- c(CR_vector,x$market_share[i])
    }
  }
  sort(CR_vector)

  for (j in 1:n){
    CR = CR + CR_vector[j]
  }
  return(CR)
}

# 2-firm concentration ratio
market_CR(market_share_data,"Smartphone",2) # returns 42
market_CR(market_share_data,"Airline",2) # returns 36
market_CR(market_share_data,"Browser",2) # returns 73
market_CR(market_share_data,"Banking",2) # returns 21

# 4-firm concentration ratio
market_CR(market_share_data,"Smartphone",4) # returns 63
market_CR(market_share_data,"Airline",4) # returns 68
market_CR(market_share_data,"Browser",4) # returns 86
market_CR(market_share_data,"Banking",4) # returns 35
```

## Functions for MOM and MLE estimates for data

I also compare the accurancy of these two methods as the value for r (the number of repeated trails) changes.

```{r}
library(nleqslv)
n <- 100
gamma_data <- rgamma(n, shape = 0.25, rate = 5)
mean(gamma_data)
var(gamma_data)

difference_MOM <- function(theta){
  alpha <- theta[1]
  beta <- theta[2]
  v1 <- (alpha/beta) - mean(gamma_data)
  v2 <- (alpha/(beta)^2) - var(gamma_data)
  output <- c(v1, v2)
  return(output)
} 
x <- c(1,1)
nleqslv(x, difference_MOM)
# when n=100, estimate using MOM for alpha is 0.2732 and for beta is 5.696.
```

```{r}
# Function for Maximum Likelihood function
n <- 100
gamma_data <- rgamma(n, shape = 0.25, rate = 5)
n <- length(gamma_data)
difference_MLE <- function(theta){
  alpha <- theta[1]
  beta <- theta[2]
  MLE <- alpha*n*log(beta) - n*log(factorial(alpha-1))+(alpha-1)*sum(log(gamma_data))-beta*sum(gamma_data)
  return(-MLE)
}
x <- c(2, 4)
optim(x,difference_MLE)
# when n=100, estimate using MLE for alpha is 0.2385 and for beta is 4.9577.
```

```{r}
# MOM r=1000
MOM_1000_alpha <- c()
MOM_1000_beta <- c()
n <- 100
for (i in 1:1000){
  x <- c(1,1)
  gamma_data <- rgamma(n, shape = 0.25, rate = 5)
  y <- nleqslv(x, difference_MOM)
  MOM_1000_alpha[i] <- y$x[1]
  MOM_1000_beta[i] <- y$x[2]
}
mean(MOM_1000_alpha) # = 0.278991
mean(MOM_1000_beta) # = 5.837994

# MLE r=1000
MLE_1000_alpha <- c()
MLE_1000_beta <- c()
n <- 100

```

```{r, echo=FALSE}
for (i in 1:1000){
  x <- c(2,4)
  gamma_data <- rgamma(n, shape = 0.25, rate = 5)
  MLE_1000_alpha[i] <- optim(x,difference_MLE)$par[1]
  MLE_1000_beta[i] <- optim(x,difference_MLE)$par[2]
}
```

```{r}
mean(MLE_1000_alpha) # = 0.2550548
mean(MLE_1000_beta) # = 5.282999
```


# The averages of MLE are closer to the true values relative to that of the MOM estimators.


```{r}
# MOM r=10000
MOM_10000_alpha <- c()
MOM_10000_beta <- c()
n <- 100
for (i in 1:10000){
  x <- c(1,1)
  gamma_data <- rgamma(n, shape = 0.25, rate = 5)
  y <- nleqslv(x, difference_MOM)
  MOM_10000_alpha[i] <- y$x[1]
  MOM_10000_beta[i] <- y$x[2]
}
mean(MOM_10000_alpha) # = 0.2784916
sd(MOM_10000_alpha) # = 0.07103596
mean(MOM_10000_beta) # = 5.808455
sd(MOM_10000_beta) # = 1.952827

# MLE r=10000
MLE_10000_alpha <- c()
MLE_10000_beta <- c()
x <- c(.1,4)
n <- 100
for (i in 1:10000){
  gamma_data <- rgamma(n, shape = 0.25, rate = 5)
  MLE_10000_alpha[i] <- optim(x,difference_MLE)$par[1]
  MLE_10000_beta[i] <- optim(x,difference_MLE)$par[2]
}
mean(MLE_10000_alpha) # = 0.2553766
sd(MLE_10000_alpha) # = 0.02889607
mean(MLE_10000_beta) # = 5.328668
sd(MLE_10000_beta) # = 1.268451
```

Comparing the standard deviations between the two types of estimators, MLE should be more efficient.

```{r}
# MOM r=1000
MOM_1000_alpha_n1000 <- c()
MOM_1000_beta_n1000 <- c()
for (i in 1:1000){
  x <- c(1,1)
  gamma_data <- rgamma(1000, shape = 0.25, rate = 5)
  y <- nleqslv(x, difference_MOM)
  MOM_1000_alpha_n1000[i] <- y$x[1]
  MOM_1000_beta_n1000[i] <- y$x[2]
}
mean(MOM_1000_alpha_n1000)
mean(MOM_1000_beta_n1000)

count <- 0
for (i in 1:length(MOM_1000_alpha_n1000)){
  if (abs(MOM_1000_alpha_n1000[i]-0.25)<0.05){
    count = count + 1
  }
}
count/length(MOM_1000_alpha_n1000) 
# returns 0.966 when n = 1000, epsilon = 0.05
# returns 1 when n = 10000, epsilon = 0.05

count2 <- 0
for (i in 1:length(MOM_1000_beta_n1000)){
  if (abs(MOM_1000_beta_n1000[i]-5)<0.5){
    count2 = count2 + 1
  }
}
count2/length(MOM_1000_beta_n1000) 
# returns 0.606 when n = 1000, epsilon = 0.5
# returns 0.997 when n = 10000, epsilon = 0.5

# When n = 1000, mean(alpha) = 0.2527, mean(beta) = 5.0663
# When n = 10000, mean(alpha) = 0.2506, mean(beta) = 5.012
```

It seems that my estimators are consistent because as n increases from 1000 to 10000, the probability for the estimators to be 0.05 or 0.5 away from the true parameters has decreased. Therefore, it is reasonable to assume that as n increases to some very large numbers, the probability will eventually decrease to zero, so the estimators appear to be consistent.Same also applies to the MLE estimators below.

```{r}
# MLE r=1000
n <- 1000
MLE_1000_alpha_n <- c()
MLE_1000_beta_n <- c()
x <- c(2,4)
for (i in 1:1000){
  gamma_data <- rgamma(n, shape = 0.25, rate = 5)
  MLE_1000_alpha_n[i] <- (optim(x,difference_MLE))$par[1]
  MLE_1000_beta_n[i] <- (optim(x,difference_MLE))$par[2]
}


count <- 0
for (i in 1:length(MLE_1000_alpha_n)){
  if (abs(MLE_1000_alpha_n[i]-0.25)<0.05){
    count = count + 1
  }
}
count/length(MLE_1000_alpha_n) 
# returns 1 when n = 1000, epsilon = 0.05
# returns 1 when n = 10000, epsilon = 0.05

count2 <- 0
for (i in 1:length(MLE_1000_beta_n)){
  if (abs(MLE_1000_beta_n[i]-5)<0.5){
    count2 = count2 + 1
  }
}
count2/length(MLE_1000_beta_n) 
# returns 0.817 when n = 1000, epsilon = 0.5
# returns 1 when n = 10000, epsilon = 0.5


mean(MLE_1000_alpha_n)
mean(MLE_1000_beta_n)
# When n = 1000, mean(alpha) = 0.25013, mean(beta) = 5.0247
# When n = 10000, mean(alpha) = 0.25006, mean(beta) = 4.9998
```

## Compare estimated parameters for linear regression models and MOM estimates.

```{r}
management_data <- read_csv("panel_data.csv")
fit <- lm(lsales ~ management, management_data)
fit
```

The independent variable has a coefficient of 0.7553, meaning that for one score point increase in the score rating of a company's management practices, the company's sales will increase by 75.53 percent.

```{r}
management_data[complete.cases(management_data), ]
b0 <- summary(fit)$coefficients[1,1]
b1 <- summary(fit)$coefficients[2,1]
betas <- c(b0, b1)
difference_MOM2 <- function(betas){
  beta0 <- betas[1]
  beta1 <- betas[2]
  yi <- management_data$lsales
  xi <- management_data$management
  v1 <- mean(yi-beta0-beta1*xi, na.rm = TRUE)
  v2 <- mean((yi-beta0-beta1*xi)*xi, na.rm = TRUE)
  output <- c(v1, v2)
  return(output)
} 
x <- c(8,0.5)
nleqslv(x, difference_MOM2) 
```

This gives an estimate of 8.5179 for beta0 and 0.7553 for beta1, which is the exact same as the estimate given by the linear regression model.
